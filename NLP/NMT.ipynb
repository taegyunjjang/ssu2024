{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    source_data_path=\"data/eng-fra.txt\",\n",
    "    output_data_path=\"data/simplest_eng_fra.csv\",\n",
    "    perc_train=0.7,\n",
    "    perc_val=0.15,\n",
    "    perc_test=0.15,\n",
    "    # 날짜와 경로 정보\n",
    "    dataset_csv=\"data/simplest_eng_fra.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/nmt_luong_sampling\",\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=5e-4,\n",
    "    batch_size=32,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,              \n",
    "    source_embedding_size=24, \n",
    "    target_embedding_size=24,\n",
    "    encoding_size=32,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "assert args.perc_test > 0 and (args.perc_test + args.perc_val + args.perc_train == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.source_data_path, encoding='utf-8') as fp:\n",
    "    lines = fp.readlines()\n",
    "    \n",
    "lines = [line.replace(\"\\n\", \"\").lower().split(\"\\t\") for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for english_sentence, french_sentence in lines:\n",
    "    data.append({\"english_tokens\": word_tokenize(english_sentence, language=\"english\"),\n",
    "                 \"french_tokens\": word_tokenize(french_sentence, language=\"french\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_phrases = (\n",
    "    (\"i\", \"am\"), (\"i\", \"'m\"), \n",
    "    (\"he\", \"is\"), (\"he\", \"'s\"),\n",
    "    (\"she\", \"is\"), (\"she\", \"'s\"),\n",
    "    (\"you\", \"are\"), (\"you\", \"'re\"),\n",
    "    (\"we\", \"are\"), (\"we\", \"'re\"),\n",
    "    (\"they\", \"are\"), (\"they\", \"'re\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = {phrase: [] for phrase in filter_phrases}\n",
    "for datum in data:\n",
    "    key = tuple(datum['english_tokens'][:2])\n",
    "    if key in data_subset:\n",
    "        data_subset[key].append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({('i', 'am'): 805,\n",
       "  ('i', \"'m\"): 4760,\n",
       "  ('he', 'is'): 1069,\n",
       "  ('he', \"'s\"): 787,\n",
       "  ('she', 'is'): 504,\n",
       "  ('she', \"'s\"): 316,\n",
       "  ('you', 'are'): 449,\n",
       "  ('you', \"'re\"): 2474,\n",
       "  ('we', 'are'): 181,\n",
       "  ('we', \"'re\"): 1053,\n",
       "  ('they', 'are'): 194,\n",
       "  ('they', \"'re\"): 470},\n",
       " 13062)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = {k: len(v) for k,v in data_subset.items()}\n",
    "counts, sum(counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "\n",
    "dataset_stage3 = []\n",
    "for phrase, datum_list in sorted(data_subset.items()):\n",
    "    np.random.shuffle(datum_list)\n",
    "    n_train = int(len(datum_list) * args.perc_train)\n",
    "    n_val = int(len(datum_list) * args.perc_val)\n",
    "\n",
    "    for datum in datum_list[:n_train]:\n",
    "        datum['split'] = 'train'\n",
    "        \n",
    "    for datum in datum_list[n_train:n_train+n_val]:\n",
    "        datum['split'] = 'val'\n",
    "        \n",
    "    for datum in datum_list[n_train+n_val:]:\n",
    "        datum['split'] = 'test'\n",
    "    \n",
    "    dataset_stage3.extend(datum_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pop and assign into the dictionary, thus modifying in place\n",
    "for datum in dataset_stage3:\n",
    "    datum['source_language'] = \" \".join(datum.pop('english_tokens'))\n",
    "    datum['target_language'] = \" \".join(datum.pop('french_tokens'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's the cutest boy in town .</td>\n",
       "      <td>c'est le garçon le plus mignon en ville .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a nonsmoker .</td>\n",
       "      <td>il est non-fumeur .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's smarter than me .</td>\n",
       "      <td>il est plus intelligent que moi .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's a lovely young man .</td>\n",
       "      <td>c'est un adorable jeune homme .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>he 's three years older than me .</td>\n",
       "      <td>il a trois ans de plus que moi .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split                    source_language  \\\n",
       "0  train     he 's the cutest boy in town .   \n",
       "1  train                he 's a nonsmoker .   \n",
       "2  train            he 's smarter than me .   \n",
       "3  train         he 's a lovely young man .   \n",
       "4  train  he 's three years older than me .   \n",
       "\n",
       "                             target_language  \n",
       "0  c'est le garçon le plus mignon en ville .  \n",
       "1                        il est non-fumeur .  \n",
       "2          il est plus intelligent que moi .  \n",
       "3            c'est un adorable jeune homme .  \n",
       "4           il a trois ans de plus que moi .  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmt_df = pd.DataFrame(dataset_stage3)\n",
    "nmt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_df.to_csv(args.output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, text_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            text_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (SurnameVectorizer): 데이터셋에서 만든 Vectorizer 객체\n",
    "        \"\"\"\n",
    "        self.text_df = text_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.text_df[self.text_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.text_df[self.text_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.text_df[self.text_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_csv):\n",
    "        \"\"\"데이터셋을 로드하고 새로운 Vectorizer를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            dataset_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            NMTDataset의 객체\n",
    "\n",
    "        \"\"\"\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        train_subset = text_df[text_df.split=='train']\n",
    "        return cls(text_df, NMTVectorizer.from_dataframe(train_subset))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath):\n",
    "        \"\"\"데이터셋과 새로운 Vectorizer 객체를 로드합니다.\n",
    "        캐싱된 Vectorizer 객체를 재사용할 때 사용합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            dataset_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            NMTDataset의 객체\n",
    "        \"\"\"\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(text_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"파일에서 Vectorizer 객체를 로드하는 정적 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 Vectorizer 객체의 위치\n",
    "        반환값:\n",
    "            NMTVectorizer의 인스턴스\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NMTVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"Vectorizer 객체를 json 형태로 디스크에 저장합니다\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트에 대한 인덱스 \n",
    "        반환값:\n",
    "            데이터 포인트(x_source, x_target, y_target, x_source_length)를 담고 있는 딕셔너리\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        # TRY IT YOURSELF\n",
    "        vector_dict = self._vectorizer.vectorize(row.source_language, row.target_language)\n",
    "        \n",
    "        return {\"x_source\": vector_dict[\"source_vector\"],\n",
    "                \"x_target\": vector_dict[\"target_x_vector\"],\n",
    "                \"y_target\": vector_dict[\"target_y_vector\"],\n",
    "                \"x_source_length\": vector_dict[\"source_length\"]}\n",
    "\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"토큰 리스트를 Vocabulary에 추가합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"토큰에 대응하는 인덱스를 추출합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "        \n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "    def __init__(self, source_vocab, target_vocab, max_source_length, max_target_length):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            source_vocab (SequenceVocabulary): 소스 단어를 정수에 매핑합니다\n",
    "            target_vocab (SequenceVocabulary): 타깃 단어를 정수에 매핑합니다\n",
    "            max_source_length (int): 소스 데이터셋에서 가장 긴 시퀀스 길이\n",
    "            max_target_length (int): 타깃 데이터셋에서 가장 긴 시퀀스 길이\n",
    "        \"\"\"\n",
    "        # TRY IT YOURSELF\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        \n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        \n",
    "\n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        \"\"\"인덱스를 벡터로 변환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            indices (list): 시퀀스를 나타내는 정수 리스트\n",
    "            vector_length (int): 인덱스 벡터의 길이\n",
    "            mask_index (int): 사용할 마스크 인덱스; 거의 항상 0\n",
    "        \"\"\"\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        # TRY IT YOURSELF\n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        vector[len(indices):] = mask_index\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def _get_source_indices(self, text):\n",
    "        \"\"\" 벡터로 변환된 소스 텍스트를 반환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            text (str): 소스 텍스트; 토큰은 공백으로 구분되어야 합니다\n",
    "        반환값:\n",
    "            indices (list): 텍스트를 표현하는 정수 리스트\n",
    "        \"\"\"\n",
    "        indices = [self.source_vocab.begin_seq_index]\n",
    "        indices.extend(self.source_vocab.lookup_token(token) for token in text.split(\" \"))\n",
    "        indices.append(self.source_vocab.end_seq_index)\n",
    "        return indices\n",
    "    \n",
    "    def _get_target_indices(self, text):\n",
    "        \"\"\" 벡터로 변환된 타깃 텍스트를 반환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            text (str): 타깃 텍스트; 토큰은 공백으로 구분되어야 합니다\n",
    "        반환값:\n",
    "            튜플: (x_indices, y_indices)\n",
    "                x_indices (list): 디코더에서 샘플을 나타내는 정수 리스트\n",
    "                y_indices (list): 디코더에서 예측을 나타내는 정수 리스트\n",
    "        \"\"\"\n",
    "        indices = [self.target_vocab.lookup_token(token) for token in text.split(\" \")]\n",
    "        x_indices = [self.target_vocab.begin_seq_index] + indices\n",
    "        y_indices = indices + [self.target_vocab.end_seq_index]\n",
    "        return x_indices, y_indices\n",
    "        \n",
    "    def vectorize(self, source_text, target_text, use_dataset_max_lengths=True):\n",
    "        \"\"\" 벡터화된 소스 텍스트와 타깃 텍스트를 반환합니다\n",
    "        \n",
    "        벡터화된 소스 텍스트는 하나의 벡터입니다.\n",
    "        벡터화된 타깃 텍스트는 7장의 성씨 모델링과 비슷한 스타일로 두 개의 벡터로 나뉩니다.\n",
    "        각 타임 스텝에서 첫 번째 벡터가 샘플이고 두 번째 벡터가 타깃이 됩니다.\n",
    "                \n",
    "        매개변수:\n",
    "            source_text (str): 소스 언어의 텍스트\n",
    "            target_text (str): 타깃 언어의 텍스트\n",
    "            use_dataset_max_lengths (bool): 최대 벡터 길이를 사용할지 여부\n",
    "        반환값:\n",
    "            다음과 같은 키에 벡터화된 데이터를 담은 딕셔너리: \n",
    "                source_vector, target_x_vector, target_y_vector, source_length\n",
    "        \"\"\"\n",
    "        source_vector_length = -1\n",
    "        target_vector_length = -1\n",
    "        \n",
    "        if use_dataset_max_lengths:\n",
    "            source_vector_length = self.max_source_length + 2\n",
    "            target_vector_length = self.max_target_length + 1\n",
    "            \n",
    "        source_indices = self._get_source_indices(source_text)\n",
    "        # TRY IT YOURSELF\n",
    "        source_vector = self._vectorize(source_indices,\n",
    "                                        vector_length=source_vector_length,\n",
    "                                        mask_index=self.source_vocab.mask_index)\n",
    "        \n",
    "        target_x_indices, target_y_indices = self._get_target_indices(target_text)\n",
    "        # TRY IT YOURSELF\n",
    "        target_x_vector = self._vectorize(target_x_indices,\n",
    "                                          vector_length=target_vector_length,\n",
    "                                          mask_index=self.target_vocab.mask_index)\n",
    "        target_y_vector = self._vectorize(target_y_indices,\n",
    "                                          vector_length=target_vector_length,\n",
    "                                          mask_index=self.target_vocab.mask_index)\n",
    "        \n",
    "        return {\"source_vector\": source_vector,\n",
    "                \"target_x_vector\": target_x_vector,\n",
    "                \"target_y_vector\": target_y_vector,\n",
    "                \"source_length\": len(source_indices)}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, bitext_df):\n",
    "        \"\"\" 데이터셋 데이터프레임으로 NMTVectorizer를 초기화합니다\n",
    "        \n",
    "        매개변수:\n",
    "            bitext_df (pandas.DataFrame): 텍스트 데이터셋\n",
    "        반환값\n",
    "        :\n",
    "            NMTVectorizer 객체\n",
    "        \"\"\"\n",
    "        # TRY IT YOURSELF\n",
    "        source_vocab = SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "        \n",
    "        max_source_length = 0\n",
    "        max_target_length = 0\n",
    "\n",
    "        for _, row in bitext_df.iterrows():\n",
    "            source_tokens = row[\"source_language\"].split(\" \")\n",
    "            if len(source_tokens) > max_source_length:\n",
    "                max_source_length = len(source_tokens)\n",
    "            for token in source_tokens:\n",
    "                source_vocab.add_token(token)\n",
    "            \n",
    "            target_tokens = row[\"target_language\"].split(\" \")\n",
    "            if len(target_tokens) > max_target_length:\n",
    "                max_target_length = len(target_tokens)\n",
    "            for token in target_tokens:\n",
    "                target_vocab.add_token(token)\n",
    "            \n",
    "        return cls(source_vocab, target_vocab, max_source_length, max_target_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        source_vocab = SequenceVocabulary.from_serializable(contents[\"source_vocab\"])\n",
    "        target_vocab = SequenceVocabulary.from_serializable(contents[\"target_vocab\"])\n",
    "        \n",
    "        return cls(source_vocab=source_vocab, \n",
    "                   target_vocab=target_vocab, \n",
    "                   max_source_length=contents[\"max_source_length\"], \n",
    "                   max_target_length=contents[\"max_target_length\"])\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"source_vocab\": self.source_vocab.to_serializable(), \n",
    "                \"target_vocab\": self.target_vocab.to_serializable(), \n",
    "                \"max_source_length\": self.max_source_length,\n",
    "                \"max_target_length\": self.max_target_length}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nmt_batches(dataset, batch_size, shuffle=True, \n",
    "                            drop_last=True, device=\"cpu\"):\n",
    "    \"\"\" 파이토치 DataLoader를 감싸고 있는 제너레이터 함수; NMT 버전 \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict['x_source_length'].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "        \n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            num_embeddings (int): 임베딩 개수는 소스 어휘 사전의 크기입니다\n",
    "            embedding_size (int): 임베딩 벡터의 크기\n",
    "            rnn_hidden_size (int): RNN 은닉 상태 벡터의 크기\n",
    "        \"\"\"\n",
    "        super(NMTEncoder, self).__init__()\n",
    "    \n",
    "        # TRY IT YOURSELF\n",
    "        self.source_embedding = nn.Embedding(num_embeddings, embedding_size, padding_idx=0)\n",
    "        self.birnn = nn.GRU(embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True)\n",
    "    \n",
    "    def forward(self, x_source, x_lengths):\n",
    "        \"\"\" 모델의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_source (torch.Tensor): 입력 데이터 텐서\n",
    "                x_source.shape는 (batch, seq_size)이다.\n",
    "            x_lengths (torch.Tensor): 배치에 있는 아이템의 길이 벡터\n",
    "        반환값:\n",
    "            튜플: x_unpacked (torch.Tensor), x_birnn_h (torch.Tensor)\n",
    "                x_unpacked.shape = (batch, seq_size, rnn_hidden_size * 2)\n",
    "                x_birnn_h.shape = (batch, rnn_hidden_size * 2)\n",
    "        \"\"\"\n",
    "        # TRY IT YOURSELF\n",
    "        x_embedded = self.source_embedding(x_source)\n",
    "        # PackedSequence 생성; x_packed.data.shape=(number_items, embedding_size)\n",
    "        x_packed = pack_padded_sequence(x_embedded, x_lengths.detach().cpu().numpy(),\n",
    "                                        batch_first=True)\n",
    "        \n",
    "        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\n",
    "        x_birnn_out, x_birnn_h = self.birnn(x_packed)\n",
    "        # (batch_size, num_rnn, feature_size)로 변환\n",
    "        x_birnn_h = x_birnn_h.permute(1, 0, 2)\n",
    "        \n",
    "        # 특성 펼침; (batch_size, num_rnn * feature_size)로 변환\n",
    "        x_birnn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\n",
    "        \n",
    "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n",
    "        \n",
    "        return x_unpacked, x_birnn_h\n",
    "\n",
    "def verbose_attention(encoder_state_vectors, query_vector):\n",
    "    \"\"\" 원소별 연산을 사용하는 어텐션 메커니즘 버전\n",
    "    \n",
    "    매개변수:\n",
    "        encoder_state_vectors (torch.Tensor): 인코더의 양방향 GRU에서 출력된 3차원 텐서\n",
    "        query_vector (torch.Tensor): 디코더 GRU의 은닉 상태\n",
    "    \"\"\"\n",
    "    # TRY IT YOURSELF\n",
    "    batch_size,  num_vectors, vector_size = encoder_state_vectors.size()\n",
    "    vector_scores = torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1, vector_size), dim=2)\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=1)\n",
    "    weighted_vectors = encoder_state_vectors * vector_probabilities.view(batch_size, num_vectors, 1)\n",
    "    context_vectors = torch.sum(weighted_vectors, dim=1)\n",
    "    \n",
    "    return context_vectors, vector_probabilities, vector_scores\n",
    "\n",
    "def terse_attention(encoder_state_vectors, query_vector):\n",
    "    \"\"\" 점곱을 사용하는 어텐션 메커니즘 버전\n",
    "    \n",
    "    매개변수:\n",
    "        encoder_state_vectors (torch.Tensor): 인코더의 양방향 GRU에서 출력된 3차원 텐서\n",
    "        query_vector (torch.Tensor): 디코더 GRU의 은닉 상태\n",
    "    \"\"\"\n",
    "    # TRY IT YOURSELF\n",
    "    vector_scores = torch.matmul(encoder_state_vectors, query_vector.unsqueeze(dim=2)).squeeze()\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=-1)\n",
    "    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),\n",
    "                                   vector_probabilities.unsqueeze(dim=2)).squeeze()\n",
    "    \n",
    "    return context_vectors, vector_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder - No Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            num_embeddings (int): 임베딩 개수는 타깃 어휘 사전에 있는 고유한 단어의 개수이다\n",
    "            embedding_size (int): 임베딩 벡터 크기\n",
    "            rnn_hidden_size (int): RNN 은닉 상태 크기\n",
    "            bos_index(int): begin-of-sequence 인덱스\n",
    "        \"\"\"\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        # TRY IT YOURSELF\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                             embedding_dim=embedding_size,\n",
    "                                             padding_idx=0)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\n",
    "        self.bos_index = bos_index\n",
    "    \n",
    "    def _init_indices(self, batch_size):\n",
    "        \"\"\" BEGIN-OF-SEQUENCE 인덱스 벡터를 반환합니다 \"\"\"\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index  # TRY IT YOURSELF\n",
    "    \n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        \"\"\" 문맥 벡터를 초기화하기 위한 0 벡터를 반환합니다 \"\"\"\n",
    "        return torch.zeros(batch_size, self._rnn_hidden_size)  # TRY IT YOURSELF\n",
    "            \n",
    "    def forward(self, encoder_state, initial_hidden_state, target_sequence):\n",
    "        \"\"\" 모델의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            encoder_state (torch.Tensor): NMTEncoder의 출력\n",
    "            initial_hidden_state (torch.Tensor): NMTEncoder의 마지막 은닉 상태\n",
    "            target_sequence (torch.Tensor): 타깃 텍스트 데이터 텐서\n",
    "        반환값:\n",
    "            output_vectors (torch.Tensor): 각 타임 스텝의 예측 벡터\n",
    "        \"\"\"    \n",
    "        # 가정: 첫 번째 차원은 배치 차원입니다\n",
    "        # 즉 입력은 (Batch, Seq)\n",
    "        # 시퀀스에 대해 반복해야 하므로 (Seq, Batch)로 차원을 바꿉니다\n",
    "        # TRY IT YOURSELF\n",
    "        target_sequence = target_sequence.permute(1, 0)\n",
    "        output_sequence_size = target_sequence.size(0)\n",
    "\n",
    "        # 주어진 인코더의 은닉 상태를 초기 은닉 상태로 사용합니다\n",
    "        # TRY IT YOURSELF\n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "        \n",
    "        batch_size = encoder_state.size(0)\n",
    "        # 문맥 벡터 0으로 초기화\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        # 첫 단어 y_t를 BOS로 초기화\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "        \n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "\n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(output_sequence_size):\n",
    "            # TRY IT YOURSELF\n",
    "            y_t_index = target_sequence[i]\n",
    "                \n",
    "            # 단계 1: 단어를 임베딩하고 이전 문맥과 연결합니다\n",
    "            # TRY IT YOURSELF\n",
    "            y_input_vector = self.target_embedding(y_t_index)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "            \n",
    "            # 단계 2: GRU를 적용하고 새로운 은닉 벡터를 얻습니다\n",
    "            # TRY IT YOURSELF\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
    "            \n",
    "            # 단계 3: 현재 은닉 상태를 사용해 인코더의 상태를 주목합니다\n",
    "            # TRY IT YOURSELF\n",
    "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state,\n",
    "                                                           query_vector=h_t)\n",
    "            \n",
    "            # 부가 작업: 시각화를 위해 어텐션 확률을 저장합니다\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            \n",
    "            # 단게 4: 현재 은닉 상태와 문맥 벡터를 사용해 다음 단어를 예측합니다\n",
    "            # TRY IT YOURSELF\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
    "            \n",
    "            # 부가 작업: 예측 성능 점수를 기록합니다\n",
    "            # TRY IT YOURSELF\n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "            \n",
    "        # TRY IT YOURSELF\n",
    "        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n",
    "        \n",
    "        return output_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder - Scheduled Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            num_embeddings (int): 임베딩 개수는 타깃 어휘 사전에 있는 고유한 단어의 개수이다\n",
    "            embedding_size (int): 임베딩 벡터 크기\n",
    "            rnn_hidden_size (int): RNN 은닉 상태 크기\n",
    "            bos_index(int): begin-of-sequence 인덱스\n",
    "        \"\"\"\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        # TRY IT YOURSELF\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(num_embeddings=num_embeddings,\n",
    "                                             embedding_dim=embedding_size,\n",
    "                                             padding_idx=0)\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\n",
    "        self.bos_index = bos_index\n",
    "        # TRY IT YOURSELF\n",
    "        self._sampling_temperature = 3\n",
    "    \n",
    "    def _init_indices(self, batch_size):\n",
    "        \"\"\" BEGIN-OF-SEQUENCE 인덱스 벡터를 반환합니다 \"\"\"\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index  # TRY IT YOURSELF\n",
    "    \n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        \"\"\" 문맥 벡터를 초기화하기 위한 0 벡터를 반환합니다 \"\"\"\n",
    "        return torch.zeros(batch_size, dtype=torch.int64) * self._rnn_hidden_size  # TRY IT YOURSELF\n",
    "            \n",
    "    def forward(self, encoder_state, initial_hidden_state, target_sequence, sample_probability=0.0):\n",
    "        \"\"\" 모델의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            encoder_state (torch.Tensor): NMTEncoder의 출력\n",
    "            initial_hidden_state (torch.Tensor): NMTEncoder의 마지막 은닉 상태\n",
    "            target_sequence (torch.Tensor): 타깃 텍스트 데이터 텐서\n",
    "            sample_probability (float): 스케줄링된 샘플링 파라미터\n",
    "                디코더 타임 스텝마다 모델 예측에 사용할 확률\n",
    "        반환값:\n",
    "            output_vectors (torch.Tensor): 각 타임 스텝의 예측 벡터\n",
    "        \"\"\"\n",
    "        if target_sequence is None:\n",
    "            sample_probability = 1.0\n",
    "        else:\n",
    "            # 가정: 첫 번째 차원은 배치 차원입니다\n",
    "            # 즉 입력은 (Batch, Seq)\n",
    "            # 시퀀스에 대해 반복해야 하므로 (Seq, Batch)로 차원을 바꿉니다\n",
    "            # TRY IT YOURSELF\n",
    "            target_sequence = target_sequence.permute(1, 0)\n",
    "            output_sequence_size = target_sequence.size(0)\n",
    "        \n",
    "        # 주어진 인코더의 은닉 상태를 초기 은닉 상태로 사용합니다\n",
    "        # TRY IT YOURSELF\n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "        \n",
    "        batch_size = encoder_state.size(0)\n",
    "        # 문맥 벡터를 0으로 초기화합니다\n",
    "        # TRY IT YOURSELF\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        \n",
    "        # 첫 단어 y_t를 BOS로 초기화합니다\n",
    "        # TRY IT YOURSELF\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "        \n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "\n",
    "        output_vectors = []\n",
    "        self._cached_p_attn = []\n",
    "        self._cached_ht = []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "        \n",
    "        for i in range(output_sequence_size):\n",
    "            # 스케줄링된 샘플링 사용 여부\n",
    "            use_sample = np.random.random() < sample_probability\n",
    "            # TRY IT YOURSELF\n",
    "            if not use_sample:\n",
    "                # TRY IT YOURSELF\n",
    "                y_t_index = target_sequence[i]\n",
    "                \n",
    "            # 단계 1: 단어를 임베딩하고 이전 문맥과 연결합니다\n",
    "            # TRY IT YOURSELF\n",
    "            y_input_vector = self.target_embedding(y_t_index)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "            \n",
    "            # 단계 2: GRU를 적용하고 새로운 은닉 벡터를 얻습니다\n",
    "            # TRY IT YOURSELF\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
    "            \n",
    "            # 단계 3: 현재 은닉 상태를 사용해 인코더의 상태를 주목합니다\n",
    "            # TRY IT YOURSELF\n",
    "            context_vectors, p_attn, _ = verbose_attention(encoder_state_vectors=encoder_state,\n",
    "                                                           query_vector=h_t)\n",
    "            \n",
    "            # 부가 작업: 시각화를 위해 어텐션 확률을 저장합니다\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "            \n",
    "            # 단게 4: 현재 은닉 상태와 문맥 벡터를 사용해 다음 단어를 예측합니다\n",
    "            # TRY IT YOURSELF\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
    "            \n",
    "            if use_sample:\n",
    "                # TRY IT YOURSELF\n",
    "                p_y_t_index = F.softmax(score_for_y_t_index * self._sampling_temperature, dim=1)\n",
    "                # _, y_t_index = torch.max(p_y_t_index, 1)\n",
    "                y_t_index = torch.multinomial(p_y_t_index, 1).squeeze()\n",
    "            \n",
    "            # 부가 작업: 예측 성능 점수를 기록합니다\n",
    "            # TRY IT YOURSELF\n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "            \n",
    "        # TRY IT YOURSELF\n",
    "        output_vectors = torch.stack(output_vectors).permute(1, 0, 2)\n",
    "        \n",
    "        return output_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    \"\"\" 신경망 기계 번역 모델 \"\"\"\n",
    "    def __init__(self, source_vocab_size, source_embedding_size, \n",
    "                 target_vocab_size, target_embedding_size, encoding_size, \n",
    "                 target_bos_index):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            source_vocab_size (int): 소스 언어에 있는 고유한 단어 개수\n",
    "            source_embedding_size (int): 소스 임베딩 벡터의 크기\n",
    "            target_vocab_size (int): 타깃 언어에 있는 고유한 단어 개수\n",
    "            target_embedding_size (int): 타깃 임베딩 벡터의 크기\n",
    "            encoding_size (int): 인코더 RNN의 크기\n",
    "            target_bos_index (int): BEGIN-OF-SEQUENCE 토큰 인덱스\n",
    "        \"\"\"\n",
    "        super(NMTModel, self).__init__()\n",
    "        # TRY IT YOURSELF\n",
    "        self.encoder = NMTEncoder(num_embeddings=source_vocab_size,\n",
    "                                  embedding_size=source_embedding_size,\n",
    "                                  rnn_hidden_size=encoding_size)\n",
    "        decoding_size = encoding_size * 2\n",
    "        self.decoder = NMTDecoder(num_embeddings=target_vocab_size,\n",
    "                                  embedding_size=target_embedding_size,\n",
    "                                  rnn_hidden_size=decoding_size,\n",
    "                                  bos_index=target_bos_index)\n",
    "    \n",
    "    def forward(self, x_source, x_source_lengths, target_sequence, sample_probability=0.0):\n",
    "        \"\"\" 모델의 정방향 계산\n",
    "        \n",
    "        매개변수:\n",
    "            x_source (torch.Tensor): 소스 텍스트 데이터 텐서\n",
    "                x_source.shape는 (batch, vectorizer.max_source_length)입니다.\n",
    "            x_source_lengths torch.Tensor): x_source에 있는 시퀀스 길이\n",
    "            target_sequence (torch.Tensor): 타깃 텍스트 데이터 텐서\n",
    "            sample_probability (float): 스케줄링된 샘플링 파라미터\n",
    "                디코더 타임 스텝마다 모델 예측에 사용할 확률\n",
    "        반환값:\n",
    "            decoded_states (torch.Tensor): 각 출력 타임 스텝의 예측 벡터\n",
    "        \"\"\"\n",
    "        # TRY IT YOURSELF\n",
    "        encoder_state, final_hidden_states = self.encoder(x_source, x_source_lengths)\n",
    "        decoded_states = self.decoder(encoder_state=encoder_state, \n",
    "                                      initial_hidden_state=final_hidden_states, \n",
    "                                      target_sequence=target_sequence)\n",
    "        \n",
    "        return decoded_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 헬퍼 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"훈련 상태 업데이트합니다.\n",
    "    \n",
    "    콤포넌트:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "         \n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= loss_tm1:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"텐서 크기 정규화\n",
    "    \n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 출력\n",
    "            3차원 텐서이면 행렬로 변환합니다.\n",
    "        y_true (torch.Tensor): 타깃 예측\n",
    "            행렬이면 벡터로 변환합니다.\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# 데이터셋과 Vectorizer\n",
    "dataset = NMTDataset.load_dataset_and_make_vectorizer(args.dataset_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# 모델\n",
    "model = NMTModel(source_vocab_size=len(vectorizer.source_vocab), \n",
    "                 source_embedding_size=args.source_embedding_size, \n",
    "                 target_vocab_size=len(vectorizer.target_vocab),\n",
    "                 target_embedding_size=args.target_embedding_size, \n",
    "                 encoding_size=args.encoding_size,\n",
    "                 target_bos_index=vectorizer.target_vocab.begin_seq_index)\n",
    "model = model.to(args.device)\n",
    "\n",
    "mask_index = vectorizer.target_vocab.mask_index\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e6de1c4b524485bd0dc309b3b63030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09ad513236044d0be0ed367f6400349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/285 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eceed67d3dc4c0b8f716777baa45480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 32 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 단계 2. 출력을 계산합니다\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_source\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m               \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_source_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m               \u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_target\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m               \u001b[49m\u001b[43msample_probability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_probability\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 단계 3. 손실을 계산합니다\u001b[39;00m\n\u001b[0;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m sequence_loss(y_pred, batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_target\u001b[39m\u001b[38;5;124m'\u001b[39m], mask_index)\n",
      "File \u001b[1;32mc:\\Users\\tgkim\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[113], line 41\u001b[0m, in \u001b[0;36mNMTModel.forward\u001b[1;34m(self, x_source, x_source_lengths, target_sequence, sample_probability)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# TRY IT YOURSELF\u001b[39;00m\n\u001b[0;32m     40\u001b[0m encoder_state, final_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_source, x_source_lengths)\n\u001b[1;32m---> 41\u001b[0m decoded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                              \u001b[49m\u001b[43minitial_hidden_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtarget_sequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_states\n",
      "File \u001b[1;32mc:\\Users\\tgkim\\anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[112], line 87\u001b[0m, in \u001b[0;36mNMTDecoder.forward\u001b[1;34m(self, encoder_state, initial_hidden_state, target_sequence, sample_probability)\u001b[0m\n\u001b[0;32m     85\u001b[0m y_input_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_embedding(y_t_index)\n\u001b[0;32m     86\u001b[0m context_vectors \u001b[38;5;241m=\u001b[39m context_vectors\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m rnn_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_input_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_vectors\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# 단계 2: GRU를 적용하고 새로운 은닉 벡터를 얻습니다\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# TRY IT YOURSELF\u001b[39;00m\n\u001b[0;32m     91\u001b[0m h_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru_cell(rnn_input, h_t)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 32 but got size 1 for tensor number 1 in the list."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                               total=args.num_epochs,\n",
    "                               position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                               total=dataset.get_num_batches(args.batch_size), \n",
    "                               position=1, \n",
    "                               leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                             total=dataset.get_num_batches(args.batch_size), \n",
    "                             position=1, \n",
    "                             leave=True)\n",
    "\n",
    "\n",
    "for epoch_index in range(args.num_epochs):\n",
    "    sample_probability = (20 + epoch_index) / args.num_epochs\n",
    "\n",
    "    train_state['epoch_index'] = epoch_index\n",
    "\n",
    "    # 훈련 세트에 대한 순회\n",
    "\n",
    "    # 훈련 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "    dataset.set_split('train')\n",
    "    batch_generator = generate_nmt_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 훈련 과정은 5단계로 이루어집니다\n",
    "\n",
    "        # --------------------------------------\n",
    "        # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 단계 2. 출력을 계산합니다\n",
    "        y_pred = model(batch_dict['x_source'], \n",
    "                       batch_dict['x_source_length'], \n",
    "                       batch_dict['x_target'],\n",
    "                       sample_probability=sample_probability)\n",
    "\n",
    "        # 단계 3. 손실을 계산합니다\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "        # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "        loss.backward()\n",
    "\n",
    "        # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "\n",
    "        # 이동 손실과 이동 정확도를 계산합니다\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # 진행 상태 막대 업데이트\n",
    "        train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                              epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "    # 검증 세트에 대한 순회\n",
    "\n",
    "    # 검증 세트와 배치 제너레이터 준비, 손실과 정확도를 0으로 설정\n",
    "    dataset.set_split('val')\n",
    "    batch_generator = generate_nmt_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    model.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # 단계 1. 출력을 계산합니다\n",
    "        y_pred = model(batch_dict['x_source'], \n",
    "                       batch_dict['x_source_length'], \n",
    "                       batch_dict['x_target'],\n",
    "                       sample_probability=sample_probability)\n",
    "\n",
    "        # 단계 2. 손실을 계산합니다\n",
    "        loss = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "        # 단계 3. 이동 손실과 이동 정확도를 계산합니다\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # 진행 상태 막대 업데이트\n",
    "        val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                        epoch=epoch_index)\n",
    "        val_bar.update()\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "\n",
    "    train_state = update_train_state(args=args, model=model, \n",
    "                                     train_state=train_state)\n",
    "\n",
    "    scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n",
    "    train_bar.n = 0\n",
    "    val_bar.n = 0\n",
    "    epoch_bar.set_postfix(best_val=train_state['early_stopping_best_val'])\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 세트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chencherry = bleu_score.SmoothingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_from_indices(indices, vocab, strict=True, return_string=True):\n",
    "    ignore_indices = set([vocab.mask_index, vocab.begin_seq_index, vocab.end_seq_index])\n",
    "    out = []\n",
    "    for index in indices:\n",
    "        if index == vocab.begin_seq_index and strict:\n",
    "            continue\n",
    "        elif index == vocab.end_seq_index and strict:\n",
    "            break\n",
    "        else:\n",
    "            out.append(vocab.lookup_index(index))\n",
    "    if return_string:\n",
    "        return \" \".join(out)\n",
    "    else:\n",
    "        return out\n",
    "    \n",
    "class NMTSampler:\n",
    "    def __init__(self, vectorizer, model):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.model = model\n",
    "    \n",
    "    def apply_to_batch(self, batch_dict):\n",
    "        self._last_batch = batch_dict\n",
    "        y_pred = self.model(x_source=batch_dict['x_source'], \n",
    "                            x_source_lengths=batch_dict['x_source_length'], \n",
    "                            target_sequence=batch_dict['x_target'])\n",
    "        self._last_batch['y_pred'] = y_pred\n",
    "        \n",
    "        attention_batched = np.stack(self.model.decoder._cached_p_attn).transpose(1, 0, 2)\n",
    "        self._last_batch['attention'] = attention_batched\n",
    "        \n",
    "    def _get_source_sentence(self, index, return_string=True):\n",
    "        indices = self._last_batch['x_source'][index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.source_vocab\n",
    "        return sentence_from_indices(indices, vocab, return_string=return_string)\n",
    "\n",
    "    def _get_reference_sentence(self, index, return_string=True):\n",
    "        indices = self._last_batch['y_target'][index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.target_vocab\n",
    "        return sentence_from_indices(indices, vocab, return_string=return_string)\n",
    "    \n",
    "    def _get_sampled_sentence(self, index, return_string=True):\n",
    "        _, all_indices = torch.max(self._last_batch['y_pred'], dim=2)\n",
    "        sentence_indices = all_indices[index].cpu().detach().numpy()\n",
    "        vocab = self.vectorizer.target_vocab\n",
    "        return sentence_from_indices(sentence_indices, vocab, return_string=return_string)\n",
    "\n",
    "    def get_ith_item(self, index, return_string=True):\n",
    "        output = {\"source\": self._get_source_sentence(index, return_string=return_string), \n",
    "                  \"reference\": self._get_reference_sentence(index, return_string=return_string), \n",
    "                  \"sampled\": self._get_sampled_sentence(index, return_string=return_string),\n",
    "                  \"attention\": self._last_batch['attention'][index]}\n",
    "        \n",
    "        reference = output['reference']\n",
    "        hypothesis = output['sampled']\n",
    "        \n",
    "        if not return_string:\n",
    "            reference = \" \".join(reference)\n",
    "            hypothesis = \" \".join(hypothesis)\n",
    "        \n",
    "        output['bleu-4'] = bleu_score.sentence_bleu(references=[reference],\n",
    "                                                    hypothesis=hypothesis,\n",
    "                                                    smoothing_function=chencherry.method1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval().to(args.device)\n",
    "\n",
    "sampler = NMTSampler(vectorizer, model)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_nmt_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "\n",
    "test_results = []\n",
    "for batch_dict in batch_generator:\n",
    "    sampler.apply_to_batch(batch_dict)\n",
    "    for i in range(args.batch_size):\n",
    "        test_results.append(sampler.get_ith_item(i, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([r['bleu-4'] for r in test_results], bins=100);\n",
    "np.mean([r['bleu-4'] for r in test_results]), np.median([r['bleu-4'] for r in test_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('val')\n",
    "batch_generator = generate_nmt_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "batch_dict = next(batch_generator)\n",
    "\n",
    "model = model.eval().to(args.device)\n",
    "sampler = NMTSampler(vectorizer, model)\n",
    "sampler.apply_to_batch(batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i in range(args.batch_size):\n",
    "    all_results.append(sampler.get_ith_item(i, False))\n",
    "    \n",
    "top_results = [x for x in all_results if x['bleu-4']>0.5]\n",
    "len(top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in top_results:\n",
    "    plt.figure()\n",
    "    target_len = len(sample['sampled'])\n",
    "    source_len = len(sample['source'])\n",
    "\n",
    "    attention_matrix = sample['attention'][:target_len, :source_len+2].transpose()\n",
    "    ax = sns.heatmap(attention_matrix, center=0.0)\n",
    "    ylabs = [\"<BOS>\"]+sample['source']+[\"<EOS>\"]\n",
    "\n",
    "    ax.set_yticklabels(ylabs, rotation=0)\n",
    "    ax.set_xticklabels(sample['sampled'], rotation=90)\n",
    "    ax.set_xlabel(\"Target Sentence\")\n",
    "    ax.set_ylabel(\"Source Sentence\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
