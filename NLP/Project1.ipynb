{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9ca1a3-fcde-4444-a98a-eb1bafd62a4c",
   "metadata": {},
   "source": [
    "## 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb5a382-61f8-45c3-b884-1a44772310cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98d753-e5e5-4ee7-ab68-2d1d3fed22e3",
   "metadata": {},
   "source": [
    "## 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7556f8-0536-4695-8cb4-e9c0fbc2e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    \"\"\" 매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "\n",
    "    def __init__(self, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token\n",
    "        \"\"\"\n",
    "        token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다 \n",
    "        \n",
    "        매개변수:\n",
    "            token(str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index(int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "\n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "\n",
    "        매개변수:\n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"인덱스 (%d)가 어휘 사전에 없습니다.\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df038035-d60a-49a6-80ae-883fc8c77901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HateVectorizer():\n",
    "    \"\"\"어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "    def __init__(self, comment_vocab, label_vocab, tokenizer, document_count, word_document_count):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            comment_vocab (Vocabulary): 토큰을 정수에 매핑하는 Vocabulary\n",
    "            label_vocab (Vocabulary): 클래스 레이블을 정수에 매핑하는 Vocabulary\n",
    "            document_count (int): 전체 문서 수\n",
    "            word_document_count (np.ndarray): 각 단어가 등장한 문서 수\n",
    "        \"\"\"\n",
    "        self.comment_vocab = comment_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.document_count = document_count\n",
    "        self.word_document_count = word_document_count\n",
    "\n",
    "    def vectorize(self, comment):\n",
    "        \"\"\"TF-IDF를 사용하여 댓글을 벡터화합니다\"\"\"\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        tokens = self.tokenizer.morphs(comment)\n",
    "        total_tokens = len(tokens)\n",
    "        token_counts = Counter(tokens)\n",
    "        \n",
    "        tf = np.zeros(len(self.comment_vocab), dtype=np.float32)\n",
    "        for token, count in token_counts.items():\n",
    "            index = self.comment_vocab.lookup_token(token)\n",
    "            tf[index] = count / total_tokens\n",
    "            \n",
    "        idf = np.log(self.document_count / (self.word_document_count + 1))\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        \n",
    "        # tf = # COMPLETE YOUR CODE\n",
    "        # idf = # COMPLETE YOUR CODE\n",
    "        \n",
    "        return tf * idf\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, comment_df, cutoff=10):\n",
    "        tokenizer = Okt()\n",
    "\n",
    "        # 1. 어휘 구축\n",
    "        comment_vocab = Vocabulary(add_unk=True)\n",
    "        label_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # count > cutoff인 단어를 어휘 사전에 추가합니다.\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        word_counts = Counter()\n",
    "        for comment in comment_df.text:\n",
    "            tokens = tokenizer.morphs(comment)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                comment_vocab.add_token(word)\n",
    "                        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "                \n",
    "        for label in sorted(set(comment_df.label)):\n",
    "            label_vocab.add_token(label)\n",
    "\n",
    "        # 2. 단어 빈도 및 문서 빈도 계산\n",
    "        document_count = len(comment_df)  # COMPLETE YOUR CODE\n",
    "        word_document_count = np.zeros(len(comment_vocab), dtype=np.float32)\n",
    "\n",
    "        for comment in comment_df.text:\n",
    "            # COMPLETE YOUR CODE - START\n",
    "            tokens = set(tokenizer.morphs(comment))\n",
    "            for token in tokens:\n",
    "                if token in comment_vocab._token_to_idx:\n",
    "                    index = comment_vocab.lookup_token(token)\n",
    "                    word_document_count[index] += 1\n",
    "        \n",
    "            # COMPLETE YOUR CODE - END\n",
    "\n",
    "        return cls(comment_vocab, label_vocab, tokenizer, document_count, word_document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa35e31-6a46-4a52-a866-77f859fe24e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HateDataset(Dataset):\n",
    "    def __init__(self, comment_df, vectorizer):\n",
    "        self.comments_df = comment_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.comments_df[self.comments_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.comments_df[self.comments_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.comments_df[self.comments_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "        \n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, comment_csv):\n",
    "        \"\"\"데이터셋을 로드하고 새로운 HateVectorizer 객체를 만듭니다\n",
    "        \n",
    "        매개변수:\n",
    "            comment_csv(str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            HateDataset의 인스턴스\n",
    "        \"\"\"\n",
    "        comment_df = pd.read_csv(comment_csv)\n",
    "        train_comment_df = comment_df[comment_df.split=='train']\n",
    "        return cls(comment_df, HateVectorizer.from_dataframe(train_comment_df))\n",
    "\n",
    "    def set_split(self, split='train'):\n",
    "        \"\"\"데이터프레임에 있는 열을 사용해 분할 세트를 선택합니다\n",
    "        \n",
    "        매개변수:\n",
    "            split (str): \"train\". \"val\", \"test\" 중 하나\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" 벡터 변환 객체를 반환합니다 \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" 파이토치 데이터셋의 주요 진입 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트의 인덱스\n",
    "        반환값:\n",
    "            데이터 포인트의 특성(x_data)와 레이블(y_target)으로 이루어진 딕셔너리\n",
    "        \"\"\"\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        row = self._target_df.iloc[index]\n",
    "        \n",
    "        comment_vector = self._vectorizer.vectorize(row.text)\n",
    "        label_index = self._vectorizer.label_vocab.lookup_token(row.label)\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        return {'x_data' : comment_vector,\n",
    "                'y_target' : label_index}\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24045ed4-155c-4234-9c96-8a8f58eeca40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, dropout_p):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "                \n",
    "        # COMPLETE YOUR CODE - END\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        indtermediate = F.relu(self.fc1(x_in))\n",
    "        indtermediate = self.dropout(indtermediate)\n",
    "        output = self.fc2(indtermediate)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b1c50-201c-42ec-80dd-8cc15e44def9",
   "metadata": {},
   "source": [
    "## 유틸함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d125b57-2e2c-48fa-9c77-9d6d7695fa70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "    걱 텐서를 지정된 장치로 이동합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = tensor.to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46673777",
   "metadata": {},
   "source": [
    "## 설정과 전처리 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51189041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용여부: False\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    comment_csv=\"hate_comments.csv\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/project1\",\n",
    "    # 모델 하이퍼파라미터\n",
    "    hidden_dim=100,\n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=42,\n",
    "    learning_rate=0.0001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=64,\n",
    "    num_epochs=10,\n",
    "    early_stopping_criteria=5,\n",
    "    # 실행 옵션\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "# CUDA 체크\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"CUDA 사용여부: {}\".format(args.cuda))\n",
    "\n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# 디렉토리 처리\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539e190-1a9a-4787-8201-e8e741440715",
   "metadata": {},
   "source": [
    "## 헬퍼함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e90aad-8912-451e-a462-b51bffd3b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_pred_indices = y_pred.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def compute_f1_score(y_pred, y_target):\n",
    "    y_pred_indices = y_pred.max(dim=1)[1]\n",
    "    return f1_score(y_target.cpu().numpy(), y_pred_indices.cpu().numpy(), average='binary')\n",
    "\n",
    "def make_train_state(args):\n",
    "        return {'stop_early': False,\n",
    "                'early_stopping_step': 0,\n",
    "                'early_stopping_best_val': 1e8,\n",
    "                'learning_rate': args.learning_rate,\n",
    "                'epoch_index': 0,\n",
    "                'train_loss': [],\n",
    "                'train_acc': [],\n",
    "                'train_f1' : [],\n",
    "                'val_loss': [],\n",
    "                'val_acc': [],\n",
    "                'val_f1': [],\n",
    "                'test_loss': None,\n",
    "                'test_acc': None,\n",
    "                'test_f1': None,\n",
    "                'model_filename': args.model_state_file\n",
    "            }\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"훈련 상태를 업데이트합니다.\n",
    "\n",
    "    Components:\n",
    "     - 조기 종료: 과대 적합 방지\n",
    "     - 모델 체크포인트: 더 나은 모델을 저장합니다\n",
    "\n",
    "    :param args: 메인 매개변수\n",
    "    :param model: 훈련할 모델\n",
    "    :param train_state: 훈련 상태를 담은 딕셔너리\n",
    "    :returns:\n",
    "        새로운 훈련 상태\n",
    "    \"\"\"\n",
    "\n",
    "    # 적어도 한 번 모델을 저장합니다\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 성능이 향상되면 모델을 저장합니다\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 손실이 나빠지면\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # 조기 종료 단계 업데이트\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 손실이 감소하면\n",
    "        else:\n",
    "            # 최상의 모델 저장\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # 조기 종료 단계 재설정\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 확인\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec7724",
   "metadata": {},
   "source": [
    "## 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f11c9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#데이터셋 만들기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mHateDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset_and_make_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomment_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mget_vectorizer()\n\u001b[0;32m      5\u001b[0m classifier \u001b[38;5;241m=\u001b[39m MLPClassifier(num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mcomment_vocab),\n\u001b[0;32m      6\u001b[0m                            hidden_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mhidden_dim,\n\u001b[0;32m      7\u001b[0m                            num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mlabel_vocab),\n\u001b[0;32m      8\u001b[0m                            dropout_p\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdropout_p)\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mHateDataset.load_dataset_and_make_vectorizer\u001b[1;34m(cls, comment_csv)\u001b[0m\n\u001b[0;32m     30\u001b[0m comment_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(comment_csv)\n\u001b[0;32m     31\u001b[0m train_comment_df \u001b[38;5;241m=\u001b[39m comment_df[comment_df\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(comment_df, \u001b[43mHateVectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_comment_df\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[3], line 41\u001b[0m, in \u001b[0;36mHateVectorizer.from_dataframe\u001b[1;34m(cls, comment_df, cutoff)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_dataframe\u001b[39m(\u001b[38;5;28mcls\u001b[39m, comment_df, cutoff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 41\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mOkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# 1. 어휘 구축\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     comment_vocab \u001b[38;5;241m=\u001b[39m Vocabulary(add_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\tgkim\\anaconda3\\envs\\NLP\\lib\\site-packages\\konlpy\\tag\\_okt.py:51\u001b[0m, in \u001b[0;36mOkt.__init__\u001b[1;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype\u001b[38;5;241m.\u001b[39misJVMStarted():\n\u001b[1;32m---> 51\u001b[0m         \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     oktJavaPackage \u001b[38;5;241m=\u001b[39m jpype\u001b[38;5;241m.\u001b[39mJPackage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkr.lucypark.okt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     54\u001b[0m     OktInterfaceJavaClass \u001b[38;5;241m=\u001b[39m oktJavaPackage\u001b[38;5;241m.\u001b[39mOktInterface\n",
      "File \u001b[1;32mc:\\Users\\tgkim\\anaconda3\\envs\\NLP\\lib\\site-packages\\konlpy\\jvm.py:55\u001b[0m, in \u001b[0;36minit_jvm\u001b[1;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[0;32m     52\u001b[0m args \u001b[38;5;241m=\u001b[39m [javadir, os\u001b[38;5;241m.\u001b[39msep]\n\u001b[0;32m     53\u001b[0m classpath \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folder_suffix]\n\u001b[1;32m---> 55\u001b[0m jvmpath \u001b[38;5;241m=\u001b[39m jvmpath \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultJVMPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\\\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibjvm.dylib\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tgkim\\anaconda3\\envs\\NLP\\lib\\site-packages\\jpype\\_jvmfinder.py:74\u001b[0m, in \u001b[0;36mgetDefaultJVMPath\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     finder \u001b[38;5;241m=\u001b[39m LinuxJVMFinder()\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_jvm_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tgkim\\anaconda3\\envs\\NLP\\lib\\site-packages\\jpype\\_jvmfinder.py:212\u001b[0m, in \u001b[0;36mJVMFinder.get_jvm_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_notsupport_ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jvm_notsupport_ext\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m JVMNotFoundException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JVM shared library file (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    213\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound. Try setting up the JAVA_HOME \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    214\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    215\u001b[0m                            \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libfile))\n",
      "\u001b[1;31mJVMNotFoundException\u001b[0m: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "#데이터셋 만들기\n",
    "dataset = HateDataset.load_dataset_and_make_vectorizer(args.comment_csv)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = MLPClassifier(num_features=len(vectorizer.comment_vocab),\n",
    "                           hidden_dim=args.hidden_dim,\n",
    "                           num_classes=len(vectorizer.label_vocab),\n",
    "                           dropout_p=args.dropout_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56306db7",
   "metadata": {},
   "source": [
    "## 훈련 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640e03d-d497-4052-9c0b-b2377024904e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                             mode='min', factor=0.5,\n",
    "                                             patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                    total=args.num_epochs, \n",
    "                    position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train', \n",
    "                    total=dataset.get_num_batches(args.batch_size), \n",
    "                    position=1, \n",
    "                    leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val', \n",
    "                total=dataset.get_num_batches(args.batch_size), \n",
    "                position=1, \n",
    "                leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # 훈련 세트에 대한 순회\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        running_f1 = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 단계 2. 출력을 계산합니다\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "            # 단계 3. 손실을 계산합니다\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "            loss.backward()\n",
    "\n",
    "            # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "            optimizer.step()\n",
    "\n",
    "            # 정확도와 F1 점수를 계산합니다\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            f1_t = compute_f1_score(y_pred, batch_dict['y_target'])\n",
    "            running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
    "\n",
    "            # 진행 바 업데이트\n",
    "            train_bar.set_postfix(loss=running_loss, \n",
    "                                    acc=running_acc, \n",
    "                                    f1=running_f1, \n",
    "                                    epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "        train_state['train_f1'].append(running_f1)\n",
    "        \n",
    "        # 검증 세트에 대한 순회\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        running_f1 = 0.0\n",
    "        classifier.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # 단계 1. 출력을 계산합니다\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "                # 단계 2. 손실을 계산합니다\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 단계 3. 정확도와 F1 점수를 계산합니다\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                f1_t = compute_f1_score(y_pred, batch_dict['y_target'])\n",
    "                running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
    "\n",
    "                val_bar.set_postfix(loss=running_loss, acc=running_acc, f1=running_f1, epoch=epoch_index)\n",
    "                val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "        train_state['val_f1'].append(running_f1)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                        train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        # 에포크마다 훈련 및 검증 결과 출력\n",
    "        print(f\"\\nEpoch {epoch_index+1}/{args.num_epochs}\")\n",
    "        print(f\"Train Loss: {train_state['train_loss'][-1]:.4f}, Train Acc: {train_state['train_acc'][-1]:.2f}%, Train F1: {train_state['train_f1'][-1]:.4f}\")\n",
    "        print(f\"Val Loss: {train_state['val_loss'][-1]:.4f}, Val Acc: {train_state['val_acc'][-1]:.2f}%, Val F1: {train_state['val_f1'][-1]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c3fae",
   "metadata": {},
   "source": [
    "## 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "115a36ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "running_f1 = 0.0\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # 출력을 계산합니다\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "    # 손실을 계산합니다\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # 정확도를 계산합니다\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "    f1_t = compute_f1_score(y_pred, batch_dict['y_target'])\n",
    "    running_f1 += (f1_t - running_f1) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "train_state['test_f1'] = running_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))\n",
    "print(\"테스트 f1 점수: {:.2f}\".format(train_state['test_f1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
