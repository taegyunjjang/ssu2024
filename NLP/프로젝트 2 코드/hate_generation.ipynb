{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 혐오 댓글 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vocabulary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"매핑을 위해 텍스트를 처리하고 어휘 사전을 만드는 클래스 \"\"\"\n",
    "    \n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            token_to_idx (dict): 기존 토큰-인덱스 매핑 딕셔너리\n",
    "        \"\"\"\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" 직렬화할 수 있는 딕셔너리를 반환합니다 \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" 직렬화된 딕셔너리에서 Vocabulary 객체를 만듭니다 \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\" 토큰을 기반으로 매핑 딕셔너리를 업데이트합니다\n",
    "\n",
    "        매개변수:\n",
    "            token (str): Vocabulary에 추가할 토큰\n",
    "        반환값:\n",
    "            index (int): 토큰에 상응하는 정수\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "            \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"토큰 리스트를 Vocabulary에 추가합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            tokens (list): 문자열 토큰 리스트\n",
    "        반환값:\n",
    "            indices (list): 토큰 리스트에 상응되는 인덱스 리스트\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"토큰에 대응하는 인덱스를 추출합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\" 인덱스에 해당하는 토큰을 반환합니다.\n",
    "        \n",
    "        매개변수: \n",
    "            index (int): 찾을 인덱스\n",
    "        반환값:\n",
    "            token (str): 인텍스에 해당하는 토큰\n",
    "        에러:\n",
    "            KeyError: 인덱스가 Vocabulary에 없을 때 발생합니다.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, tokenizer, token_to_idx=None):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        # KLUE RoBERTa 토크나이저에서 특수 토큰 설정\n",
    "        self._mask_token = tokenizer.mask_token\n",
    "        self._unk_token = tokenizer.unk_token\n",
    "        self._begin_seq_token = tokenizer.cls_token\n",
    "        self._end_seq_token = tokenizer.sep_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        # KLUE RoBERTa 토크나이저 초기화\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "        \n",
    "        # token_to_idx만 추출하여 부모 클래스 초기화\n",
    "        token_to_idx = contents.get('token_to_idx', {})\n",
    "        return cls(tokenizer=tokenizer, token_to_idx=token_to_idx)\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\" 토큰에 대응하는 인덱스를 추출합니다.\n",
    "        토큰이 없으면 UNK 인덱스를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            token (str): 찾을 토큰 \n",
    "        반환값:\n",
    "            index (int): 토큰에 해당하는 인덱스\n",
    "        노트:\n",
    "            UNK 토큰을 사용하려면 (Vocabulary에 추가하기 위해)\n",
    "            `unk_index`가 0보다 커야 합니다.\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentVectorizer(object):\n",
    "    \"\"\" 어휘 사전을 생성하고 관리합니다 \"\"\"\n",
    "    \n",
    "    def __init__(self, text_vocab, tokenizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            text_vocab (SequenceVocabulary): 댓글 텍스트의 토큰을 정수로 매핑합니다\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_vocab = text_vocab\n",
    "\n",
    "    def vectorize(self, text, vector_length=-1):\n",
    "        \"\"\"댓글 텍스트를 샘플과 타겟 벡터로 변환합니다\n",
    "        \n",
    "        매개변수:\n",
    "            text (str): 벡터로 변경할 댓글 텍스트\n",
    "            vector_length (int): 인덱스 벡터의 길이를 맞추기 위한 매개변수\n",
    "        반환값:\n",
    "            튜플: (from_vector, to_vector)\n",
    "                from_vector (numpy.ndarray): 샘플 벡터\n",
    "                to_vector (numpy.ndarray): 타겟 벡터\n",
    "        \"\"\"\n",
    "        # RoBERTa tokenizer로 토큰화\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        # 1. 토큰을 인덱스로 변환\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        token_indices = [self.text_vocab.lookup_token(token) for token in tokens]\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "\n",
    "        # 2. from_vector 생성\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        from_indices = [self.text_vocab.begin_seq_index] + token_indices\n",
    "        from_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        from_vector[:len(from_indices)] = from_indices\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "\n",
    "        # 3. to_vector 생성\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        to_indices = token_indices + [self.text_vocab.end_seq_index]\n",
    "        to_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        to_vector[:len(to_indices)] = to_indices\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "\n",
    "        return from_vector, to_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df):\n",
    "        \"\"\"데이터셋 데이터프레임으로 객체를 초기화 합니다\n",
    "        \n",
    "        매개변수:\n",
    "            df (pandas.DataFrame): 댓글 데이터프레임\n",
    "        반환값:\n",
    "            CommentVectorizer 객체\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "        text_vocab = SequenceVocabulary(tokenizer=tokenizer)\n",
    "\n",
    "        # 텍스트 어휘 사전 구축\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        for text in df['text']:\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            text_vocab.add_many(tokens)\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "\n",
    "        return cls(text_vocab, tokenizer)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"파일에서 CommentVectorizer 객체를 초기화합니다\n",
    "        \n",
    "        매개변수:\n",
    "            contents (dict): CommentVectorizer를 위한 어휘 사전을 담은 딕셔너리\n",
    "                이 딕셔너리는 `vectorizer.to_serializable()`를 사용해 만듭니다\n",
    "        반환값:\n",
    "            CommentVectorizer 객체\n",
    "        \"\"\"\n",
    "        # KLUE RoBERTa 토크나이저 초기화\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "        \n",
    "        text_vocab = SequenceVocabulary.from_serializable(contents['text_vocab'])\n",
    "        return cls(text_vocab=text_vocab, tokenizer=tokenizer)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"직렬화된 결과를 반환합니다\"\"\"\n",
    "        return {\n",
    "            'text_vocab': self.text_vocab.to_serializable()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이토치 데이터셋 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, comment_df, vectorizer):\n",
    "        \"\"\"\n",
    "        매개변수:\n",
    "            comment_df (pandas.DataFrame): 데이터셋\n",
    "            vectorizer (CommentVectorizer): 데이터셋에서 만든 Vectorizer 객체\n",
    "        \"\"\"\n",
    "        self.comment_df = comment_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        # 최대 시퀀스 길이 계산\n",
    "        self._max_seq_length = max(len(self._vectorizer.tokenizer.tokenize(text))\n",
    "            for text in self.comment_df['text']) + 2  # COMPLETE YOUR CODE\n",
    "\n",
    "        # 데이터셋 분할\n",
    "        self.train_df = self.comment_df[self.comment_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.comment_df[self.comment_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.comment_df[self.comment_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        # 데이터셋 조회 딕셔너리 생성\n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val': (self.val_df, self.validation_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_csv):\n",
    "        \"\"\"데이터셋을 로드하고 새로운 Vectorizer를 만듭니다.\n",
    "        \n",
    "        매개변수:\n",
    "            dataset_csv (str): 데이터셋의 위치\n",
    "        반환값:\n",
    "            CommentDataset 객체\n",
    "        \"\"\"\n",
    "        comment_df = pd.read_csv(dataset_csv)\n",
    "        vectorizer = CommentVectorizer.from_dataframe(comment_df)\n",
    "        return cls(comment_df, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath):\n",
    "        \"\"\"데이터셋과 캐싱된 Vectorizer 객체를 로드합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            dataset_csv (str): 데이터셋의 위치\n",
    "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
    "        반환값:\n",
    "            CommentDataset 객체\n",
    "        \"\"\"\n",
    "        comment_df = pd.read_csv(dataset_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(comment_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"파일에서 Vectorizer 객체를 로드하는 정적 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): 직렬화된 Vectorizer 객체의 위치\n",
    "        반환값:\n",
    "            CommentVectorizer 객체\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CommentVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"Vectorizer 객체를 json 형태로 디스크에 저장합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            vectorizer_filepath (str): Vectorizer 객체의 저장 위치\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"벡터 변환 객체를 반환합니다\"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"파이토치 데이터셋의 주요 진입 메서드\n",
    "        \n",
    "        매개변수:\n",
    "            index (int): 데이터 포인트에 대한 인덱스 \n",
    "        반환값:\n",
    "            데이터 포인트(x_data, y_target)를 담고 있는 딕셔너리\n",
    "        \"\"\"\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        row = self._target_df.iloc[index]\n",
    "        from_vector, to_vector = self._vectorizer.vectorize(row['text'], self._max_seq_length)\n",
    "\n",
    "        # COMPLETE YOUR CODE - END\n",
    "\n",
    "        return {\n",
    "            'x_data': from_vector,\n",
    "            'y_target': to_vector\n",
    "        }\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"배치 크기가 주어지면 데이터셋으로 만들 수 있는 배치 개수를 반환합니다.\n",
    "        \n",
    "        매개변수:\n",
    "            batch_size (int)\n",
    "        반환값:\n",
    "            배치 개수\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    파이토치 DataLoader를 감싸고 있는 제너레이터 함수.\n",
    "    걱 텐서를 지정된 장치로 이동합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLayerLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, dropout_p,\n",
    "                 hidden_size, embedding_path, padding_idx=0):\n",
    "        \"\"\"단층 LSTM 기반의 생성 모델\n",
    "        \n",
    "        매개변수:\n",
    "            embedding_dim (int): KLUE RoBERTa 임베딩 차원\n",
    "            num_embeddings (int): 임베딩 테이블 크기 (단어장 크기)\n",
    "            hidden_size (int): LSTM의 은닉 상태 크기\n",
    "            embedding_path (str): 정렬된 임베딩 파일 경로\n",
    "            padding_idx (int): 패딩 토큰의 인덱스\n",
    "            dropout_p (float): 드롭아웃 확률\n",
    "        \"\"\"\n",
    "        super(SingleLayerLSTMModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.load(embedding_path),\n",
    "            freeze=True,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        # COMPLETE YOUR CODE - START\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, num_embeddings)\n",
    "                \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"순전파\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 텐서 (batch_size, sequence_length)\n",
    "            apply_softmax (bool): 소프트맥스 적용 여부\n",
    "        반환값:\n",
    "            output (torch.Tensor): 출력 텐서 (batch_size, sequence_length, num_embeddings)\n",
    "        \"\"\"\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        embedded = self.embedding(x_in)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        output = self.fc(lstm_out)\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=-1)\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        return output\n",
    "\n",
    "\n",
    "class SingleLayerGRUModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, dropout_p,\n",
    "                 hidden_size, embedding_path, padding_idx=0):\n",
    "        \"\"\"단층 GRU 기반의 생성 모델\n",
    "        \n",
    "        매개변수:\n",
    "            embedding_dim (int): KLUE RoBERTa 임베딩 차원\n",
    "            num_embeddings (int): 임베딩 테이블 크기 (단어장 크기)\n",
    "            hidden_size (int): GRU의 은닉 상태 크기\n",
    "            embedding_path (str): 정렬된 임베딩 파일 경로\n",
    "            padding_idx (int): 패딩 토큰의 인덱스\n",
    "            dropout_p (float): 드롭아웃 확률\n",
    "        \"\"\"\n",
    "        super(SingleLayerGRUModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.load(embedding_path),\n",
    "            freeze=True,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        # COMPLETE YOUR CODE - START\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, num_embeddings)\n",
    "                \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"순전파\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 텐서 (batch_size, sequence_length)\n",
    "            apply_softmax (bool): 소프트맥스 적용 여부\n",
    "        반환값:\n",
    "            output (torch.Tensor): 출력 텐서 (batch_size, sequence_length, num_embeddings)\n",
    "        \"\"\"\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        embedded = self.embedding(x_in)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        output = self.fc(gru_out)\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=-1)\n",
    "        \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiLayerGRUModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, dropout_p,\n",
    "                 hidden_size, num_layers, embedding_path, padding_idx=0):\n",
    "        \"\"\"다층 GRU 기반의 생성 모델\n",
    "        \n",
    "        매개변수:\n",
    "            embedding_dim (int): KLUE RoBERTa 임베딩 차원\n",
    "            num_embeddings (int): 임베딩 테이블 크기 (단어장 크기)\n",
    "            hidden_size (int): GRU의 은닉 상태 크기\n",
    "            num_layers (int): GRU 층의 개수\n",
    "            embedding_path (str): 정렬된 임베딩 파일 경로\n",
    "            padding_idx (int): 패딩 토큰의 인덱스\n",
    "            dropout_p (float): 드롭아웃 확률\n",
    "        \"\"\"\n",
    "        super(MultiLayerGRUModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.load(embedding_path),\n",
    "            freeze=True,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        # COMPLETE YOUR CODE - START\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_p if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_embeddings)\n",
    "                \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"순전파\n",
    "        \n",
    "        매개변수:\n",
    "            x_in (torch.Tensor): 입력 텐서 (batch_size, sequence_length)\n",
    "            apply_softmax (bool): 소프트맥스 적용 여부\n",
    "        반환값:\n",
    "            output (torch.Tensor): 출력 텐서 (batch_size, sequence_length, num_embeddings)\n",
    "        \"\"\"\n",
    "        # COMPLETE YOUR CODE - START\n",
    "        embedded = self.embedding(x_in)\n",
    "        gru_out, _ = self.gru(embedded)\n",
    "        output = self.fc(gru_out)\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=-1)\n",
    "            \n",
    "        # COMPLETE YOUR CODE - END\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 헬퍼 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"텐서 크기 정규화\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 출력\n",
    "            3차원 텐서이면 2차원 행렬로 변환합니다.\n",
    "        y_true (torch.Tensor): 타깃 텐서\n",
    "            2차원 텐서이면 1차원 벡터로 변환합니다.\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    \"\"\"정확도 계산\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 예측 결과\n",
    "        y_true (torch.Tensor): 실제 정답\n",
    "        mask_index (int): 마스크 토큰의 인덱스\n",
    "    반환값:\n",
    "        정확도 (float)\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "\n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "\n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    \"\"\"시퀀스 손실 계산\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 예측 결과\n",
    "        y_true (torch.Tensor): 실제 정답\n",
    "        mask_index (int): 마스크 토큰의 인덱스\n",
    "    반환값:\n",
    "        손실 값 (torch.Tensor)\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "def compute_perplexity(y_pred, y_true, mask_index):\n",
    "    \"\"\"Perplexity 계산 함수\n",
    "\n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 예측 결과\n",
    "        y_true (torch.Tensor): 실제 정답\n",
    "        mask_index (int): 마스크 토큰의 인덱스\n",
    "    반환값:\n",
    "        perplexity (float): Perplexity 값\n",
    "    \"\"\"\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    loss = F.cross_entropy(y_pred, y_true, ignore_index=mask_index, reduction='sum')\n",
    "    n_tokens = torch.ne(y_true, mask_index).sum().item()\n",
    "    perplexity = math.exp(loss.item() / n_tokens)\n",
    "    return perplexity\n",
    "\n",
    "def make_train_state(args):\n",
    "    \"\"\"훈련 상태 초기화\n",
    "\n",
    "    매개변수:\n",
    "        args: 설정 값들이 담긴 Namespace 객체\n",
    "    반환값:\n",
    "        훈련 상태를 담은 딕셔너리\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'stop_early': False,\n",
    "        'early_stopping_step': 0,\n",
    "        'early_stopping_best_val': 1e8,\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'epoch_index': 0,\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'train_ppl': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'val_ppl': [],\n",
    "        'test_loss': -1,\n",
    "        'test_acc': -1,\n",
    "        'test_ppl': -1,\n",
    "        'model_filename': args.model_state_file\n",
    "    }\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"훈련 상태 업데이트\n",
    "\n",
    "    매개변수:\n",
    "        args: 설정 값들이 담긴 Namespace 객체\n",
    "        model: 학습 중인 모델\n",
    "        train_state: 현재 훈련 상태 딕셔너리\n",
    "    반환값:\n",
    "        업데이트된 훈련 상태 딕셔너리\n",
    "    \"\"\"\n",
    "    # 첫 번째 에포크에서는 무조건 모델을 저장\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # 그 이후 에포크에서는 성능 개선 여부에 따라 모델 저장\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # 검증 손실이 증가하면 조기 종료 단계 증가\n",
    "        if loss_t >= loss_tm1:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # 검증 손실이 감소하면 모델 저장 및 조기 종료 단계 초기화\n",
    "        else:\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # 조기 종료 여부 판단\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_align_roberta_embedding(vectorizer, args):\n",
    "    \"\"\"\n",
    "    KLUE RoBERTa 임베딩을 저장하고 text_vocab에 맞게 정렬 후 재저장합니다.\n",
    "    파일이 존재하면 기존 임베딩을 로드합니다.\n",
    "    \n",
    "    매개변수:\n",
    "        vectorizer (CommentVectorizer): 데이터셋 기반의 Vectorizer 객체\n",
    "        args (Namespace): 설정 값을 담은 객체 (embedding_path 포함)\n",
    "    \"\"\"\n",
    "    # 이미 정렬된 임베딩 파일이 존재하면 로드\n",
    "    if os.path.exists(args.embedding_path):\n",
    "        print(f\"임베딩 파일 로드 중: {args.embedding_path}\")\n",
    "        aligned_embedding = torch.load(args.embedding_path)\n",
    "        return aligned_embedding\n",
    "\n",
    "    print(f\"새로운 KLUE RoBERTa 임베딩 생성 중...\")\n",
    "    \n",
    "    # KLUE RoBERTa 토크나이저 및 모델 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n",
    "    model_roberta = AutoModel.from_pretrained(\"klue/roberta-base\")\n",
    "    roberta_embedding = model_roberta.get_input_embeddings().weight.data\n",
    "\n",
    "    # 새로운 정렬된 임베딩 초기화\n",
    "    text_vocab = vectorizer.text_vocab\n",
    "    aligned_embedding = torch.zeros(len(text_vocab), roberta_embedding.size(1))\n",
    "\n",
    "    for idx in range(len(text_vocab)):\n",
    "        token = text_vocab.lookup_index(idx)\n",
    "\n",
    "        # 토큰 ID 가져와 임베딩 매핑\n",
    "        if token in tokenizer.get_vocab():\n",
    "            roberta_idx = tokenizer.convert_tokens_to_ids(token)\n",
    "            aligned_embedding[idx] = roberta_embedding[roberta_idx]\n",
    "        else:\n",
    "            # KLUE RoBERTa에 없는 토큰은 랜덤 초기화\n",
    "            aligned_embedding[idx] = torch.randn(roberta_embedding.size(1))\n",
    "\n",
    "    # 정렬된 임베딩 저장\n",
    "    torch.save(aligned_embedding, args.embedding_path)\n",
    "    print(f\"KLUE RoBERTa 임베딩 저장 완료 및 정렬됨: {args.embedding_path}\")\n",
    "    return aligned_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1. 설정 및 초기화 중...\")\n",
    "args = Namespace(\n",
    "    # 파일 경로\n",
    "    comment_csv=\"hate_comments.csv\",\n",
    "    save_dir=\"model_storage/hate_comment_generation\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    embedding_path=\"model_storage/hate_comment_generation/roberta_embedding.pt\",\n",
    "    lstm_model_file=\"lstm_model.pth\",\n",
    "    single_gru_model_file=\"single_gru_model.pth\",\n",
    "    multi_gru_model_file=\"multi_gru_model.pth\",\n",
    "    \n",
    "    # 모델 하이퍼파라미터\n",
    "    embedding_dim=768,  # KLUE RoBERTa 임베딩 차원\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    dropout_p=0.1,\n",
    "    \n",
    "    # 훈련 하이퍼파라미터\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    \n",
    "    # 실행 옵션\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "model_files = {\n",
    "    'SingleLayerLSTM': args.lstm_model_file,\n",
    "    'SingleLayerGRU': args.single_gru_model_file,\n",
    "    'MultiLayerGRU': args.multi_gru_model_file\n",
    "}\n",
    "\n",
    "# 경로 확장\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    for model_name in model_files:\n",
    "        model_files[model_name] = os.path.join(args.save_dir, model_files[model_name])\n",
    "        \n",
    "    print(\"\\n파일 경로:\")\n",
    "    print(f\"Vectorizer: {args.vectorizer_file}\")\n",
    "    print(\"\\n저장될 모델 파일:\")\n",
    "    for model_name, path in model_files.items():\n",
    "        print(f\"{model_name}: {path}\")\n",
    "\n",
    "# CUDA 설정\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"CUDA 사용 여부: {}\".format(args.cuda))\n",
    "\n",
    "# 재현성을 위한 시드 설정 및 디렉토리 생성\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2. 데이터 로드 중...\")\n",
    "print(f\"데이터셋 파일: {args.comment_csv}\")\n",
    "\n",
    "# 데이터셋과 벡터라이저 로드 또는 생성\n",
    "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
    "    # 체크포인트에서 데이터 로드\n",
    "    print(\"\\n체크포인트에서 데이터 로드 중...\")\n",
    "    dataset = CommentDataset.load_dataset_and_load_vectorizer(\n",
    "        args.comment_csv, args.vectorizer_file)\n",
    "else:\n",
    "    # 새로운 데이터셋과 벡터라이저 생성\n",
    "    print(\"\\n데이터셋 생성 중...\")\n",
    "    dataset = CommentDataset.load_dataset_and_make_vectorizer(args.comment_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "print(f\"데이터셋 크기: {len(dataset):,} 샘플\")\n",
    "print(f\"어휘 사전 크기: {len(vectorizer.text_vocab):,} 토큰\")\n",
    "\n",
    "print(\"\\n3. CUDA 설정 확인 중...\")\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(f\"학습 장치: {args.device}\")\n",
    "\n",
    "# KLUE RoBERTa 임베딩 저장 및 정렬\n",
    "save_and_align_roberta_embedding(vectorizer, args)\n",
    "\n",
    "print(\"\\n4. 모델 초기화 중...\")\n",
    "# 모델 초기화 및 학습\n",
    "models = {\n",
    "    'SingleLayerLSTM': SingleLayerLSTMModel(\n",
    "        embedding_dim=args.embedding_dim,\n",
    "        num_embeddings=len(vectorizer.text_vocab),\n",
    "        hidden_size=args.hidden_size,\n",
    "        padding_idx=vectorizer.text_vocab.mask_index,\n",
    "        dropout_p=args.dropout_p,\n",
    "        embedding_path=args.embedding_path\n",
    "    ),\n",
    "    'SingleLayerGRU': SingleLayerGRUModel(\n",
    "        embedding_dim=args.embedding_dim,\n",
    "        num_embeddings=len(vectorizer.text_vocab),\n",
    "        hidden_size=args.hidden_size,\n",
    "        padding_idx=vectorizer.text_vocab.mask_index,\n",
    "        dropout_p=args.dropout_p,\n",
    "        embedding_path=args.embedding_path\n",
    "    ),\n",
    "    'MultiLayerGRU': MultiLayerGRUModel(\n",
    "        embedding_dim=args.embedding_dim,\n",
    "        num_embeddings=len(vectorizer.text_vocab),\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_layers=args.num_layers,\n",
    "        padding_idx=vectorizer.text_vocab.mask_index,\n",
    "        dropout_p=args.dropout_p,\n",
    "        embedding_path=args.embedding_path\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"\\n5. 모델 학습 시작...\")\n",
    "train_states = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== {model_name} 모델 학습 ===\")\n",
    "    model = model.to(args.device)\n",
    "    \n",
    "    # 모델별 train_state 초기화\n",
    "    args_copy = copy.deepcopy(args)\n",
    "    args_copy.model_state_file = model_files[model_name]  # 각 모델별 저장 경로 설정\n",
    "    train_state = make_train_state(args_copy)\n",
    "    train_states[model_name] = train_state\n",
    "    \n",
    "    print(f\"모델이 저장될 경로: {train_state['model_filename']}\")\n",
    "    \n",
    "    # 옵티마이저 설정\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=optimizer, mode='min', factor=0.5, patience=1\n",
    "    )\n",
    "\n",
    "    # tqdm을 사용한 진행 상태 막대 설정\n",
    "    epoch_bar = tqdm(desc='에포크 진행', total=args.num_epochs, position=0)\n",
    "    dataset.set_split('train')\n",
    "    train_bar = tqdm(desc='훈련 중', total=dataset.get_num_batches(args.batch_size),\n",
    "                     position=1, leave=True)\n",
    "    dataset.set_split('val')\n",
    "    val_bar = tqdm(desc='검증 중', total=dataset.get_num_batches(args.batch_size),\n",
    "                   position=1, leave=True)\n",
    "\n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # 훈련 세트에 대한 순회\n",
    "            model.train()\n",
    "            dataset.set_split('train')\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            running_ppl = 0.0\n",
    "            batch_generator = generate_batches(dataset, args.batch_size, device=args.device)\n",
    "\n",
    "            # 진행 상태 막대 초기화\n",
    "            train_bar.reset()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # 단계 1. 그레이디언트를 0으로 초기화합니다\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 단계 2. 출력을 계산합니다\n",
    "                y_pred = model(\n",
    "                    x_in=batch_dict['x_data']\n",
    "                )\n",
    "\n",
    "                # 단계 3. 손실을 계산합니다\n",
    "                loss = sequence_loss(y_pred, batch_dict['y_target'],\n",
    "                                     vectorizer.text_vocab.mask_index)\n",
    "\n",
    "                # 단계 4. 손실을 사용해 그레이디언트를 계산합니다\n",
    "                loss.backward()\n",
    "\n",
    "                # 단계 5. 옵티마이저로 가중치를 업데이트합니다\n",
    "                optimizer.step()\n",
    "\n",
    "                # 이동 손실과 이동 정확도를 계산합니다\n",
    "                running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'],\n",
    "                                         vectorizer.text_vocab.mask_index)\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                ppl_t = compute_perplexity(y_pred, batch_dict['y_target'],\n",
    "                                           vectorizer.text_vocab.mask_index)\n",
    "                running_ppl += (ppl_t - running_ppl) / (batch_index + 1)\n",
    "\n",
    "                # 진행 상태 막대 업데이트\n",
    "                train_bar.set_postfix(loss=running_loss, acc=running_acc, ppl=running_ppl)\n",
    "                train_bar.update()\n",
    "\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "            train_state['train_acc'].append(running_acc)\n",
    "            train_state['train_ppl'].append(running_ppl)\n",
    "\n",
    "            # 검증 세트에 대한 순회\n",
    "            model.eval()\n",
    "            dataset.set_split('val')\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            running_ppl = 0.0\n",
    "            batch_generator = generate_batches(dataset, args.batch_size, device=args.device)\n",
    "\n",
    "            # 진행 상태 막대 초기화\n",
    "            val_bar.reset()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                    # 단계 1. 출력을 계산합니다\n",
    "                    y_pred = model(\n",
    "                        x_in=batch_dict['x_data']\n",
    "                    )\n",
    "\n",
    "                    # 단계 2. 손실을 계산합니다\n",
    "                    loss = sequence_loss(y_pred, batch_dict['y_target'],\n",
    "                                         vectorizer.text_vocab.mask_index)\n",
    "\n",
    "                    # 단계 3. 이동 손실과 이동 정확도를 계산합니다\n",
    "                    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "                    acc_t = compute_accuracy(y_pred, batch_dict['y_target'],\n",
    "                                             vectorizer.text_vocab.mask_index)\n",
    "                    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                    ppl_t = compute_perplexity(y_pred, batch_dict['y_target'],\n",
    "                                               vectorizer.text_vocab.mask_index)\n",
    "                    running_ppl += (ppl_t - running_ppl) / (batch_index + 1)\n",
    "\n",
    "                    # 진행 상태 막대 업데이트\n",
    "                    val_bar.set_postfix(loss=running_loss, acc=running_acc, ppl=running_ppl)\n",
    "                    val_bar.update()\n",
    "\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "            train_state['val_ppl'].append(running_ppl)\n",
    "\n",
    "            # 학습 상태 업데이트 및 조기 종료 확인\n",
    "            train_state = update_train_state(args=args_copy, model=model, \n",
    "                                           train_state=train_state)\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "            # 에포크 진행 상태 막대 업데이트\n",
    "            epoch_bar.update()\n",
    "            epoch_bar.set_postfix(epoch=epoch_index)\n",
    "\n",
    "            # 조기 종료 여부 확인\n",
    "            if train_state['stop_early']:\n",
    "                print(f\"\\n{model_name} 모델 조기 종료!\")\n",
    "                break\n",
    "\n",
    "        train_states[model_name] = train_state\n",
    "        print(f\"\\n{model_name} 모델 학습 완료!\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n{model_name} 모델 학습 중단!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 세트 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n6. 각 모델 테스트 시작...\")\n",
    "test_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n=== {model_name} 모델 테스트 ===\")\n",
    "    \n",
    "    # 저장된 모델 가중치를 로드하고 장치로 이동\n",
    "    model.load_state_dict(torch.load(train_states[model_name]['model_filename']))\n",
    "    model = model.to(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # 테스트 데이터셋 설정\n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset, args.batch_size, device=args.device)\n",
    "\n",
    "    # 진행 상태 막대 설정\n",
    "    test_bar = tqdm(desc=f'{model_name} 테스트 중', total=dataset.get_num_batches(args.batch_size), position=0)\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_ppl = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 모델 예측\n",
    "            y_pred = model(\n",
    "                x_in=batch_dict['x_data']\n",
    "            )\n",
    "\n",
    "            # 손실 계산\n",
    "            loss = sequence_loss(y_pred, batch_dict['y_target'],\n",
    "                                 vectorizer.text_vocab.mask_index)\n",
    "\n",
    "            # 정확도 계산\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'],\n",
    "                                     vectorizer.text_vocab.mask_index)\n",
    "\n",
    "            # Perplexity 계산\n",
    "            ppl_t = compute_perplexity(y_pred, batch_dict['y_target'],\n",
    "                                       vectorizer.text_vocab.mask_index)\n",
    "\n",
    "            # 손실과 정확도 누적\n",
    "            test_loss += (loss.item() - test_loss) / (batch_index + 1)\n",
    "            test_acc += (acc_t - test_acc) / (batch_index + 1)\n",
    "            test_ppl += (ppl_t - test_ppl) / (batch_index + 1)\n",
    "\n",
    "            # 진행 상태 막대 업데이트\n",
    "            test_bar.set_postfix(loss=test_loss, acc=test_acc, ppl=test_ppl)\n",
    "            test_bar.update()\n",
    "\n",
    "    test_results[model_name] = {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_acc,\n",
    "        'perplexity': test_ppl\n",
    "    }\n",
    "    \n",
    "    train_states[model_name]['test_loss'] = test_loss\n",
    "    train_states[model_name]['test_acc'] = test_acc\n",
    "    train_states[model_name]['test_ppl'] = test_ppl\n",
    "\n",
    "    test_bar.close()\n",
    "\n",
    "    print(f\"\\n{model_name} 테스트 결과:\")\n",
    "    print(f\"Loss: {test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Perplexity: {test_ppl:.2f}\")\n",
    "\n",
    "# 모델 성능 비교 출력\n",
    "print(\"\\n=== 모델 성능 비교 ===\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'모델':^20} {'Loss':^12} {'Accuracy':^12} {'Perplexity':^12}\")\n",
    "print(\"=\"*60)\n",
    "for model_name, results in test_results.items():\n",
    "    print(f\"{model_name:^20} {results['loss']:^12.4f} {results['accuracy']:^12.2f} {results['perplexity']:^12.2f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
